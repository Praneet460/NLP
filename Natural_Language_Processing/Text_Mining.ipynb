{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started with Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us take text1\n",
    "text1 = \"Ethics are built right into the ideals and objectives of the United Nations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out number of characters in text1\n",
    "len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ethics', 'are', 'built', 'right', 'into', 'the', 'ideals', 'and', 'objectives', 'of', 'the', 'United', 'Nations']\n",
      "\n",
      "\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# find out the number of words\n",
    "text2 = text1.split(' ')\n",
    "print(text2)\n",
    "print('\\n')\n",
    "print(len(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ethics', 'built', 'right', 'into', 'ideals', 'objectives', 'United', 'Nations']\n",
      "\n",
      "\n",
      "['are', 'the', 'and', 'of', 'the']\n",
      "\n",
      "\n",
      "['Ethics', 'United', 'Nations']\n",
      "\n",
      "\n",
      "['Ethics', 'ideals', 'objectives', 'Nations']\n"
     ]
    }
   ],
   "source": [
    "# find words in text1 which are 3 letters long\n",
    "print([w for w in text2 if len(w) > 3])\n",
    "print('\\n')\n",
    "# find words in text1 which are less than 4 letters\n",
    "print([w for w in text2 if len(w) < 4])\n",
    "print('\\n')\n",
    "# find capatilized words in text1\n",
    "print([w for w in text2 if w.istitle()])\n",
    "print('\\n')\n",
    "# find words in text1 which ends with 's'\n",
    "print([w for w in text2 if w.endswith('s')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'be', 'not', 'or', 'to'}\n",
      "\n",
      "\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# lets consider another text\n",
    "text3 = 'To be or not to be'\n",
    "# find unique words in text3 and the number of unique words\n",
    "text4 = text3.split(' ')\n",
    "print(set([w.lower() for w in text4])) # set is used to find unique words\n",
    "print('\\n')\n",
    "# number of unique words\n",
    "print(len(set([w.lower() for w in text4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ethics', 'are', 'into', 'ideals', 'and', 'built', 'nations', 'the', 'united', 'right', 'objectives', 'of'}\n",
      "\n",
      "\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# finding number of unique words in text1 and name them\n",
    "print(set([w.lower() for w in text2]))\n",
    "print('\\n')\n",
    "print(len(set([w.lower() for w in text2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'agad', 'g', '']\n",
      "\n",
      "\n",
      "ouagadougou\n",
      "\n",
      "\n",
      "['o', 'u', 'a', 'g', 'a', 'd', 'o', 'u', 'g', 'o', 'u']\n"
     ]
    }
   ],
   "source": [
    "# let us consider another text\n",
    "text5 = 'ouagadougou'\n",
    "# split is using 'ou'\n",
    "text6 = text5.split('ou')\n",
    "print(text6)\n",
    "print('\\n')\n",
    "# lets rejoin them\n",
    "print(('ou'.join(text6)))\n",
    "print('\\n')\n",
    "# Split word into characters\n",
    "print(list(text5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A quick brown fox jumped over the lazy dog.\n",
      "\n",
      "\n",
      "['', '', '', '', 'A', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog.']\n",
      "\n",
      "\n",
      "A quick brown fox jumped over the lazy dog.\n",
      "\n",
      "\n",
      "['A', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog.']\n"
     ]
    }
   ],
   "source": [
    "# Cleaning of text\n",
    "text8 = '    A quick brown fox jumped over the lazy dog.'\n",
    "print(text8)\n",
    "print('\\n')\n",
    "# split the text\n",
    "print(text8.split(' '))\n",
    "print('\\n')\n",
    "# Removing white spaces\n",
    "text9 = text8.strip()\n",
    "print(text9)\n",
    "print('\\n')\n",
    "print(text9.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "\n",
      "\n",
      "40\n",
      "\n",
      "\n",
      "A quick brOwn fOx jumped Over the lazy dOg.\n"
     ]
    }
   ],
   "source": [
    "# Finding a particular letter in text9\n",
    "print(text9.find('o'))\n",
    "print('\\n')\n",
    "# finding the letter from the last\n",
    "print(text9.rfind('o'))\n",
    "print('\\n')\n",
    "# Replacing the letter 'o' with 'O' in text 9\n",
    "print(text9.replace('o', 'O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Declaration of Human Rights \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Handling Files\n",
    "f = open('eng.txt', 'r')\n",
    "text10 = f.readline()\n",
    "print(text10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10910\n",
      "\n",
      "\n",
      "90\n",
      "\n",
      "\n",
      "Universal Declaration of Human Rights \n"
     ]
    }
   ],
   "source": [
    "# Reading full file\n",
    "f.seek(0)\n",
    "text12 = f.read()\n",
    "print(len(text12))\n",
    "print('\\n')\n",
    "text13 = text12.splitlines()\n",
    "print(len(text13))\n",
    "print('\\n')\n",
    "print(text13[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867\n",
      "\n",
      "\n",
      "612\n"
     ]
    }
   ],
   "source": [
    "text14 =  text12.strip()\n",
    "# finding number of unique words in text14\n",
    "text15 = text14.split(' ')\n",
    "print(len(text15))\n",
    "print('\\n')\n",
    "text16 = [w.lower() for w in text15]\n",
    "text17 = set(text16)\n",
    "text18 = len(text17)\n",
    "print(text18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ethics are built right into the ideals and objectives of the United Nations\" #UNSG @ NY Society for Ethical Culture bit.ly/2guVelr @UN @UN_Women\n"
     ]
    }
   ],
   "source": [
    "# Let us consider a tweet\n",
    "text20 = '\"Ethics are built right into the ideals and objectives of the United Nations\" #UNSG @ NY Society for Ethical Culture bit.ly/2guVelr @UN @UN_Women'\n",
    "print(text20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text21 = text20.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#UNSG']\n"
     ]
    }
   ],
   "source": [
    "# find all hashtags\n",
    "text22 = [ w for w in text21 if w.startswith('#')]\n",
    "print(text22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@UN', '@UN_Women']\n"
     ]
    }
   ],
   "source": [
    "#find all Callouts\n",
    "import re\n",
    "text23 = [ w for w in text21 if re.search('@[A-Za-z0-9_]+', w)]\n",
    "print(text23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'u', 'a', 'a', 'o', 'u', 'o', 'u']\n",
      "\n",
      "\n",
      "['g', 'd', 'g']\n"
     ]
    }
   ],
   "source": [
    "# find all the vowels in text5\n",
    "print(re.findall(r'[aeiou]', text5))\n",
    "print('\\n')\n",
    "# find letters which are not vowels and present in text5\n",
    "print(re.findall(r'[^aeiou]', text5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Text Data in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monday: the doctor's appointment is at 2:45pm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuesday: The dentist's appointment is at 11:30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wednesday: At 7:00pm, there is a basketball game!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday: Be back dome by 11:15 pm at the latest.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday: Take the train at 08:10 am, arrive at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0     Monday: the doctor's appointment is at 2:45pm.\n",
       "1  Tuesday: The dentist's appointment is at 11:30...\n",
       "2  Wednesday: At 7:00pm, there is a basketball game!\n",
       "3  Thursday: Be back dome by 11:15 pm at the latest.\n",
       "4  Friday: Take the train at 08:10 am, arrive at ..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time_sentences = [\"Monday: the doctor's appointment is at 2:45pm.\",\n",
    "            \"Tuesday: The dentist's appointment is at 11:30 am.\",\n",
    "            \"Wednesday: At 7:00pm, there is a basketball game!\",\n",
    "            \"Thursday: Be back dome by 11:15 pm at the latest.\",\n",
    "            \"Friday: Take the train at 08:10 am, arrive at 09:00am.\"]\n",
    "df = pd.DataFrame(time_sentences, columns=['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    46\n",
       "1    50\n",
       "2    49\n",
       "3    49\n",
       "4    54\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of chareacters in each string\n",
    "df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7\n",
       "1     8\n",
       "2     8\n",
       "3    10\n",
       "4    10\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of words in each string\n",
    "df['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: text, dtype: bool"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which entries contain the world appointment\n",
    "df['text'].str.contains('appointment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    4\n",
       "2    3\n",
       "3    4\n",
       "4    8\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the nuber of occurances of a digit in each string\n",
    "df['text'].str.count(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [2, 4, 5]\n",
       "1                [1, 1, 3, 0]\n",
       "2                   [7, 0, 0]\n",
       "3                [1, 1, 1, 5]\n",
       "4    [0, 8, 1, 0, 0, 9, 0, 0]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].str.findall(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [(2, 45)]\n",
       "1              [(11, 30)]\n",
       "2               [(7, 00)]\n",
       "3              [(11, 15)]\n",
       "4    [(08, 10), (09, 00)]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting out hours and minutes in each string\n",
    "df['text'].str.findall(r'(\\d?\\d):(\\d\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          ???: the doctor's appointment is at 2:45pm.\n",
       "1       ???: The dentist's appointment is at 11:30 am.\n",
       "2          ???: At 7:00pm, there is a basketball game!\n",
       "3         ???: Be back dome by 11:15 pm at the latest.\n",
       "4    ???: Take the train at 08:10 am, arrive at 09:...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace weekdays with '???'\n",
    "df['text'].str.replace(r'\\w+day', '???')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Mon: the doctor's appointment is at 2:45pm.\n",
       "1       Tue: The dentist's appointment is at 11:30 am.\n",
       "2          Wed: At 7:00pm, there is a basketball game!\n",
       "3         Thu: Be back dome by 11:15 pm at the latest.\n",
       "4    Fri: Take the train at 08:10 am, arrive at 09:...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace with first 3 letters of weekdays\n",
    "df['text'].str.replace(r'(\\w+day)', lambda x: x.groups()[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0   2  45\n",
       "1  11  30\n",
       "2   7  00\n",
       "3  11  15\n",
       "4  08  10"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract hours and minutes in form of dataframe\n",
    "df['text'].str.extract(r'(\\d?\\d):(\\d?\\d)', expand = True)\n",
    "# This only return first match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>2:45pm</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>11:30 am</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>7:00pm</td>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <td>11:15 pm</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>08:10 am</td>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09:00am</td>\n",
       "      <td>09</td>\n",
       "      <td>00</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0   1   2   3\n",
       "  match                      \n",
       "0 0        2:45pm   2  45  pm\n",
       "1 0      11:30 am  11  30  am\n",
       "2 0        7:00pm   7  00  pm\n",
       "3 0      11:15 pm  11  15  pm\n",
       "4 0      08:10 am  08  10  am\n",
       "  1       09:00am  09  00  am"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To extract all use extractall\n",
    "df['text'].str.extractall(r'((\\d?\\d):(\\d?\\d) ?(am|pm))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>00</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>08</td>\n",
       "      <td>10</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09</td>\n",
       "      <td>00</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hour minute period\n",
       "  match                   \n",
       "0 0        2     45     pm\n",
       "1 0       11     30     am\n",
       "2 0        7     00     pm\n",
       "3 0       11     15     pm\n",
       "4 0       08     10     am\n",
       "  1       09     00     am"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name the colums of the above Dataframe\n",
    "df['text'].str.extractall(r'(?P<hour>\\d?\\d):(?P<minute>\\d\\d) ?(?P<period>[ap]m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: The Man Who Was Thursday by G . K . Chesterton 1908>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at text9\n",
    "text9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "# Have a look at sentences\n",
    "sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "\n",
      "\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(sent7)\n",
    "print('\\n')\n",
    "print(len(sent7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100676\n",
      "\n",
      "\n",
      "12408\n"
     ]
    }
   ],
   "source": [
    "print(len(text7))\n",
    "# Unique words\n",
    "print('\\n')\n",
    "print(len(set(text7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Desai', 'stockpiles', 'Ringers', 'merit', 'notion', 'something', 'partners', 'limbo', 'insists', 'check', 'gunship', 'fear', 'index-arbitrage', 'assembly', 'neighbors', 'skidded', 'Namibia', '*-118', 'Islamabad', 'possessed']\n"
     ]
    }
   ],
   "source": [
    "# First 20 words from text7\n",
    "print(list(set(text7))[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2789\n"
     ]
    }
   ],
   "source": [
    "# Frequency of words\n",
    "dist = FreqDist(text3)\n",
    "print(len(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1 = dist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['father', 'Abraham', 'Joseph']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqwords = [w for w in vocab1 if len(w) > 5 and dist[w] > 100]\n",
    "freqwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list', 'listed', 'lists', 'listing', 'listings']\n",
      "\n",
      "\n",
      "['list', 'list', 'list', 'list', 'list']\n"
     ]
    }
   ],
   "source": [
    "## Stemming\n",
    "input1 = \"List listed lists listing listings\"\n",
    "words1 = input1.lower().split(' ')\n",
    "print(words1)\n",
    "print('\\n')\n",
    "porter = nltk.PorterStemmer()\n",
    "print([porter.stem(w) for w in words1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the', 'inherent', 'dignity', 'and', 'of', 'the', 'equal', 'and', 'inalienable', 'rights', 'of']\n",
      "\n",
      "\n",
      "['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the', 'inherent', 'dignity', 'and', 'of', 'the', 'equal', 'and', 'inalienable', 'right', 'of']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization : Do stemming but stems are all valid words\n",
    "udhr =  nltk.corpus.udhr.words('English-Latin1')\n",
    "udhr1 = udhr[:20]\n",
    "print(udhr1)\n",
    "print('\\n')\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "print([lemma.lemmatize(w) for w in udhr1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']\n",
      "\n",
      "\n",
      "['Children', 'should', \"n't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "text31 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text32 = text31.split(' ')\n",
    "print(text32)\n",
    "print('\\n')\n",
    "# That's why we use tokenization\n",
    "text33 = nltk.word_tokenize(text31)\n",
    "print(text33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\n",
      "\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "['This is the first sentence.', 'A gallon of milk in the U.S. costs $2.99.', 'Is this the third sentence?', 'Yes, it is!']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Splitter\n",
    "text34 =\"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "print(text34)\n",
    "print('\\n')\n",
    "sentences = nltk.sent_tokenize(text34)\n",
    "print(len(sentences))\n",
    "print('\\n')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('CC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text51 = \"Children shouldn't drink a sugary drink before bed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text52 = nltk.word_tokenize(text51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Children', 'should', \"n't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Children', 'NNP'), ('should', 'MD'), (\"n't\", 'RB'), ('drink', 'VB'), ('a', 'DT'), ('sugary', 'JJ'), ('drink', 'NN'), ('before', 'IN'), ('bed', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text53 =nltk.pos_tag(text52)\n",
    "print(text53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Visiting', 'VBG'), ('aunts', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('a', 'DT'), ('nuisance', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Ambiguity in POS Tagging\n",
    "text54 = \"Visiting aunts can be a nuisance\"\n",
    "text55 = nltk.word_tokenize(text54)\n",
    "text56 = nltk.pos_tag(text55)\n",
    "print(text56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'loves', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "# Parsing Sentence Structure\n",
    "text57 = \"Alice loves Bob\"\n",
    "text58 = nltk.word_tokenize(text57)\n",
    "print(text58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 'NNP'), ('loves', 'VBZ'), ('Bob', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text59 = nltk.pos_tag(text58)\n",
    "print(text59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also defined the grammar by own\n",
    "grammar = nltk.CFG.fromstring(\"\"\"S -> NP VP\n",
    "                                VP -> V NP\n",
    "                                NP -> 'Alice' | 'Bob'\n",
    "                                V -> 'loves'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.ChartParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "trees = parser.parse_all(text58)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('a', 'DT'), ('telescope', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Ambiguity in Parsing\n",
    "text60 = \"I saw the man with a telescope\"\n",
    "text61 = nltk.word_tokenize(text60)\n",
    "text62 = nltk.pos_tag(text61)\n",
    "print(text62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also define the grammer by own of text60\n",
    "grammar_text60 = nltk.CFG.fromstring(\"\"\"S -> NP VP\n",
    "                                        VP -> V NP | VP PP\n",
    "                                        PP -> P NP\n",
    "                                        NP -> DT N | DT N PP | 'I'\n",
    "                                        DT -> 'a' | 'the'\n",
    "                                        N -> 'man' | 'telescope'\n",
    "                                        V -> 'saw'\n",
    "                                        P -> 'with'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_text60 = nltk.ChartParser(grammar_text60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V saw) (NP (DT the) (N man)))\n",
      "    (PP (P with) (NP (DT a) (N telescope)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (DT the) (N man) (PP (P with) (NP (DT a) (N telescope))))))\n"
     ]
    }
   ],
   "source": [
    "trees_text60 =parser_text60.parse_all(text61)\n",
    "for tree_text60 in trees_text60:\n",
    "    print(tree_text60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "# Parse Tree Collection\n",
    "from nltk.corpus import treebank\n",
    "text71 = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "print(text71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topic identification : Is this news article about Politics, Sports, or Technology?\n",
    "##### Spelling Correction: weather or whether? / color or colour? \n",
    "##### Sentiment analysis: Is this movie review positive or negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Types of Text Features\n",
    "Words (STOP WORDS : Are commonly occuring word like 'the', 'a',\n",
    "       Normalization : Make lower case vs leave as-is like 'us' or 'US')\n",
    " Characteristics of words: Capatilization,\n",
    " Part of speech of words in a sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
